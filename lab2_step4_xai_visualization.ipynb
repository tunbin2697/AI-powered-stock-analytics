{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tải thư viện chưa có tren collab"
      ],
      "metadata": {
        "id": "kuIQk4QRCU9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install feedparser\n",
        "!pip install ta\n",
        "!pip install transformers\n",
        "!pip install joblib\n",
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoLzMpM5CTMj",
        "outputId": "b0894dcc-ca8b-44ff-dd85-c94ae7c4d6e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.36)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.3.0)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.4.0)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.14.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.4.26)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.11/dist-packages (6.0.11)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: ta in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas->ta) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import thư viện"
      ],
      "metadata": {
        "id": "l99FFftJRQS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import feedparser\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import ta\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import joblib"
      ],
      "metadata": {
        "id": "Xjoyk0g2RPRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cào dữ liệu"
      ],
      "metadata": {
        "id": "Ato0psFPMgBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cào stock data"
      ],
      "metadata": {
        "id": "Xx9PiTza6L9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aapl = yf.Ticker(\"AAPL\")\n",
        "df = aapl.history(start=\"1980-12-12\", end=\"2025-06-07\", interval=\"1d\")\n",
        "\n",
        "# Xoá 2 cột: 'Dividends' và 'Stock Splits'\n",
        "data_cleaned = df.drop(columns=['Dividends', 'Stock Splits'])\n",
        "\n",
        "# Lưu vào file CSV\n",
        "output_path = 'AAPL.csv'\n",
        "data_cleaned.to_csv(output_path)\n"
      ],
      "metadata": {
        "id": "Cd-Pn9BTtF4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRQ0tNcPPNnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cào dữ liệu tin tức tài chính"
      ],
      "metadata": {
        "id": "PD3GFN3R6OM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_yahoo_finance_rss(ticker=\"AAPL\"):\n",
        "    rss_url = f\"https://feeds.finance.yahoo.com/rss/2.0/headline?s={ticker}&region=US&lang=en-US\"\n",
        "    feed = feedparser.parse(rss_url)\n",
        "\n",
        "    articles = []\n",
        "    for entry in feed.entries:\n",
        "        articles.append({\n",
        "            \"title\": entry.title,\n",
        "            \"link\": entry.link,\n",
        "            \"published\": entry.published,\n",
        "            \"summary\": entry.summary\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv(f\"{ticker}_yahoo_news.csv\", index=False)\n",
        "    print(f\"✅ Lưu {len(df)} bài viết vào file: {ticker}_yahoo_news.csv\")\n",
        "\n",
        "# Test chạy hàm:\n",
        "fetch_yahoo_finance_rss(\"AAPL\")"
      ],
      "metadata": {
        "id": "5xlOJse7Rzqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Đọc dữ liệu"
      ],
      "metadata": {
        "id": "0pV9i6h8MjYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock = pd.read_csv('AAPL.csv')\n",
        "stock.head()"
      ],
      "metadata": {
        "id": "_Ue3Gym5KbcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "x2E-65u3MmNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thống kê missing & invalid."
      ],
      "metadata": {
        "id": "bGAPIto-MsV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lọc các dòng có giá trị missing (NaN)\n",
        "rows_with_missing = stock[stock.isnull().any(axis=1)]\n",
        "missing_count = len(rows_with_missing)\n",
        "\n",
        "# Lọc các dòng có giá trị invalid (chuỗi rỗng)\n",
        "rows_with_invalid = stock[(stock == '').any(axis=1)]\n",
        "invalid_count = len(rows_with_invalid)\n",
        "\n",
        "# In kết quả\n",
        "print(\"Missing Count:\", missing_count)\n",
        "print(\"Invalid Count:\", invalid_count)\n",
        "\n",
        "# In các dòng có missing\n",
        "if missing_count > 0:\n",
        "    print(\"Rows with Missing:\")\n",
        "    print(rows_with_missing)\n",
        "\n",
        "# In các dòng có invalid\n",
        "if invalid_count > 0:\n",
        "    print(\"Rows with Invalid:\")\n",
        "    print(rows_with_invalid)\n"
      ],
      "metadata": {
        "id": "gtqmBILRLWMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SMA50 và SMA200"
      ],
      "metadata": {
        "id": "MT6J4We8xgnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock['SMA50'] = stock['Close'].rolling(window=50).mean()\n",
        "stock['SMA200'] = stock['Close'].rolling(window=200).mean()\n",
        "\n",
        "# Ghi dữ liệu đã cập nhật trở lại vào file CSV (ghi đè)\n",
        "stock.to_csv('AAPL.csv', index=False)\n",
        "stock.tail()"
      ],
      "metadata": {
        "id": "A71CUq72xl57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RSI"
      ],
      "metadata": {
        "id": "tqjvA53vyXaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock['RSI'] = ta.momentum.RSIIndicator(stock['Close'], window=14).rsi()\n",
        "stock.to_csv('AAPL.csv', index=False)\n",
        "stock.tail()"
      ],
      "metadata": {
        "id": "JeiEl4vwyZKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lag-1 và Lag-2"
      ],
      "metadata": {
        "id": "YkTOVmFMzAHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock['Close_Lag_1'] = stock['Close'].shift(1)\n",
        "stock['Close_Lag_2'] = stock['Close'].shift(2)\n",
        "stock.to_csv('AAPL.csv', index=False)\n",
        "stock.head()"
      ],
      "metadata": {
        "id": "6_v5Ogeqyz6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rolling Mean và Rolling Std"
      ],
      "metadata": {
        "id": "xOdKSGZrzEfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock['Close_Rolling_Mean_5'] = stock['Close'].rolling(window=5).mean()\n",
        "stock['Close_Rolling_Std_5'] = stock['Close'].rolling(window=5).std()\n",
        "stock.to_csv('AAPL.csv', index=False)\n",
        "stock.head(7)"
      ],
      "metadata": {
        "id": "-LjM6gyry6h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fill Nan (sử dụng KNN imputation)"
      ],
      "metadata": {
        "id": "A523o-fk0FWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_df = stock.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Áp dụng KNN imputation với k = 5\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "imputed_data = imputer.fit_transform(numeric_df)\n",
        "\n",
        "# Gán lại vào dataframe ban đầu (giữ nguyên cột Date)\n",
        "stock[numeric_df.columns] = imputed_data\n",
        "stock.to_csv('AAPL.csv', index=False)\n",
        "\n",
        "stock.head()\n"
      ],
      "metadata": {
        "id": "20dAT8hL0RWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FinBERT\n",
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Load dữ liệu\n",
        "df = pd.read_csv(\"AAPL_yahoo_news.csv\")\n",
        "texts = df['summary'].fillna(\"\").tolist()\n",
        "\n",
        "# Hàm lấy embedding FinBERT (768 chiều)\n",
        "def get_finbert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    last_hidden_state = outputs.last_hidden_state.squeeze(0)\n",
        "    attention_mask = inputs['attention_mask'].squeeze(0)\n",
        "    masked_embeddings = last_hidden_state * attention_mask.unsqueeze(1)\n",
        "    mean_embedding = masked_embeddings.sum(0) / attention_mask.sum()\n",
        "    return mean_embedding.numpy()\n",
        "\n",
        "# Tính embedding\n",
        "embeddings = [get_finbert_embedding(text) for text in texts]\n",
        "\n",
        "# Lưu file CSV chỉ chứa embedding\n",
        "embedding_df = pd.DataFrame(embeddings)\n",
        "embedding_df.to_csv(\"AAPL_yahoo_news.csv\", index=False)\n",
        "\n",
        "# Lưu file .pkl chứa ánh xạ bằng joblib\n",
        "encoding_key = {\n",
        "    \"embedding\": embeddings,\n",
        "    \"original_summary\": texts\n",
        "}\n",
        "joblib.dump(encoding_key, \"AAPL_yahoo_news_key.pkl\")"
      ],
      "metadata": {
        "id": "M8Ki9vEK85h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install captum\n",
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "L5Rsh5p9OXqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from captum.attr import IntegratedGradients\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create visuals directory if it doesn't exist\n",
        "os.makedirs('visuals', exist_ok=True)"
      ],
      "metadata": {
        "id": "hnVsA7MWUN3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DAVOTS Analysis Functions\n",
        "Implementing Data Attribution Visualization Over Time Series using Integrated Gradients"
      ],
      "metadata": {
        "id": "-_W3opdlV0xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_feature_attributions(model, features, target):\n",
        "    ig = IntegratedGradients(model)\n",
        "    attributions = ig.attribute(features, target=target)\n",
        "    return attributions.detach().numpy()\n",
        "\n",
        "def create_davots_heatmap(attributions, feature_names, timestamps):\n",
        "    # Normalize attributions to percentages\n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_attrs = scaler.fit_transform(attributions) * 100\n",
        "\n",
        "    fig = go.Figure(data=go.Heatmap(\n",
        "        z=normalized_attrs,\n",
        "        x=feature_names,\n",
        "        y=timestamps,\n",
        "        colorscale='Viridis',\n",
        "        text=np.round(normalized_attrs, 1),\n",
        "        texttemplate='%{text}%',\n",
        "        textfont={\"size\":10},\n",
        "        colorbar=dict(title='Attribution %')\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='DAVOTS Feature Attribution Heatmap',\n",
        "        xaxis_title='Features',\n",
        "        yaxis_title='Timestamp',\n",
        "        height=800\n",
        "    )\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "5XML4xBVVu0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ICFTS Analysis Functions\n",
        "Implementing Interventional Counterfactual Time Series analysis"
      ],
      "metadata": {
        "id": "86QwBlAEV8CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_counterfactuals(sentiment_series, perturbations=[0.05, 0.10]):\n",
        "    counterfactuals = {}\n",
        "    for p in perturbations:\n",
        "        # Generate positive and negative perturbations\n",
        "        counterfactuals[f'+{int(p*100)}%'] = sentiment_series * (1 + p)\n",
        "        counterfactuals[f'-{int(p*100)}%'] = sentiment_series * (1 - p)\n",
        "    return counterfactuals\n",
        "\n",
        "def create_icfts_plot(base_prices, counterfactual_results, timestamps):\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Plot base price\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=timestamps,\n",
        "        y=base_prices,\n",
        "        name='Base Price',\n",
        "        line=dict(color='black')\n",
        "    ))\n",
        "\n",
        "    colors = ['red', 'pink', 'lightblue', 'blue']\n",
        "    for (name, prices), color in zip(counterfactual_results.items(), colors):\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=timestamps,\n",
        "            y=prices,\n",
        "            name=f'Sentiment {name}',\n",
        "            line=dict(color=color)\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='ICFTS: Price Changes with Sentiment Perturbations',\n",
        "        xaxis_title='Time',\n",
        "        yaxis_title='Price',\n",
        "        height=600\n",
        "    )\n",
        "\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "5Vq8UZi7V7sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute Visualizations\n",
        "Create and save all required visualizations"
      ],
      "metadata": {
        "id": "rqGir1R9WBsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data preparation (you'll need to adapt this to your actual model data)\n",
        "feature_names = ['Price', 'Volume', 'Sentiment', 'Macro']\n",
        "timestamps = pd.to_datetime(stock['Date'].iloc[-30:]).dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Generate some sample attributions (replace with actual model outputs)\n",
        "attributions = np.random.rand(30, 4)  # 30 days, 4 features\n",
        "\n",
        "# Create and save DAVOTS heatmap\n",
        "davots_fig = create_davots_heatmap(attributions, feature_names, timestamps)\n",
        "davots_fig.write_html('visuals/davots_heatmap.html')\n",
        "\n",
        "# Create and save ICFTS causal plot\n",
        "base_prices = stock['Close'].iloc[-30:].values\n",
        "counterfactual_results = generate_counterfactuals(base_prices)\n",
        "icfts_fig = create_icfts_plot(base_prices, counterfactual_results, timestamps)\n",
        "icfts_fig.write_html('visuals/icfts_causal_plot.html')\n",
        "\n",
        "# Create price vs sentiment line plot\n",
        "line_fig = px.line(stock[-30:], x=stock[-30:].index, y=['Close', 'RSI'],\n",
        "                  title='Price vs Sentiment Indicator')\n",
        "line_fig.write_html('visuals/line_price_sentiment.html')\n",
        "\n",
        "# Create returns histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "stock['Returns'] = stock['Close'].pct_change()\n",
        "stock['Returns'].hist(bins=50)\n",
        "plt.title('Distribution of Returns')\n",
        "plt.xlabel('Returns')\n",
        "plt.ylabel('Frequency')\n",
        "plt.savefig('visuals/histogram_returns.png')\n",
        "plt.close()\n",
        "\n",
        "# Create word cloud from news\n",
        "text = ' '.join(df['summary'].fillna(''))\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig('visuals/wordcloud_news.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "ZNWLQyJ4WEPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}